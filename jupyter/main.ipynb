{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage du text\n",
    "\n",
    "Etapes de néttoyage:\n",
    "1. Mettre en minuscle le texte\n",
    "2. Enlever les contractions\n",
    "3. Enlever les espaces\n",
    "4. Tokeniser le texte\n",
    "5. Lemmatiser les mots\n",
    "6. Enlever les éléments ininteréssant (chiffres, ponctuation, mots non anglais et lettres isolées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L.R. Brane loves his life - his car, his apartment, his job, but especially his girlfriend, Vespa. One day while showering, Vespa runs out of shampoo. L.R. runs across the street to a convenience store to buy some more, a quick trip of no more than a few minutes. When he returns, Vespa is gone and every trace of her existence has been wiped out. L.R.'s life becomes a tortured existence as one strange event after another occurs to confirm in his mind that a conspiracy is working against his finding Vespa.\n",
      "-----------------------------------------------------\n",
      "brane love life car apartment job especially girlfriend vespa day showering vespa run shampoo run across street convenience store buy quick trip minute return vespa gone every trace existence ha wiped life becomes tortured existence strange event another occurs confirm mind conspiracy working finding vespa\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.externals._packaging.version import parse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import argparse\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
    "                   'eight', 'nine', 'go', 'goes', 'get', 'also', 'however', 'tells']\n",
    "stopwords_list += [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "stopwords_list += [x for x in string.ascii_lowercase]\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('\\d+', ' ', text)\n",
    "\n",
    "    text = text.strip(' ')\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stem_sentence = []\n",
    "    for word in text:\n",
    "        stem = lemmatizer.lemmatize(word)\n",
    "        stem_sentence.append(stem)\n",
    "\n",
    "    text = [w for w in stem_sentence if w not in stopwords_list]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # to_csv(\"archive/dataset/test_data.txt\", \"test_data_clean.csv\")\n",
    "    # to_csv_train_solution(\"archive/dataset/test_data_solution.txt\", \"test_data_solution_clean.csv\")\n",
    "    with open(\"text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = \" \".join(f.readlines())\n",
    "    print(text)\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    clean_text = clean_text(text)\n",
    "    print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage de la base de donnée\n",
    "\n",
    "L'une des première étapes étant de nettoyer notre base de données.\n",
    "\n",
    "Pour se faire nous avons transformer la base de donnée de base en txt vers csv tout en nettoyant le text avec la fonction clean_text pour que ce soit plus facile à manipuler.\n",
    "\n",
    "**Base de donnée txt:**\n",
    "\n",
    "![DB txt](./img/clean_db/db_txt.png)\n",
    "\n",
    "**Base de donnée csv nettoyée:**\n",
    "\n",
    "![DB csv](./img/clean_db/db_csv_clean.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(file, output):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    df = pd.DataFrame(columns=[\"id\", \"title\", \"description\"])\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].split(\":::\")\n",
    "\n",
    "        df_tmp = pd.DataFrame([(int(line[0]), line[1], clean_text(line[2]))], columns=[\"id\", \"title\", \"description\"])\n",
    "        df = df.append(df_tmp)\n",
    "    df.to_csv(output)\n",
    "    print(output, \"Saved\")\n",
    "\n",
    "def to_csv_train_solution(file, output):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    df = pd.DataFrame(columns=[\"id\", \"title\", \"genre\", \"description\"])\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].split(\":::\")\n",
    "\n",
    "        df_tmp = pd.DataFrame([(int(line[0]), line[1], line[2], clean_text(line[3]))], columns=[\"id\", \"title\", \"genre\", \"description\"])\n",
    "        df = df.append(df_tmp)\n",
    "    df.to_csv(output)\n",
    "    print(output, \"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fréquence des mots\n",
    "\n",
    "En regardant la fréquence des mots par genre nous avons pu remarquer qu’il varié selon ce dernier.\n",
    "\n",
    "Ça nous a donc donné notre première idée de classifieur, la classification par champs lexical.\n",
    "\n",
    "Exemples de fréquence de mot par genre:\n",
    "\n",
    "Drama :\n",
    "\n",
    "![Drama](./img/freq_word/drama%20.png)\n",
    "\n",
    "Comedy :\n",
    "\n",
    "![Comedy](./img/freq_word/comedy%20.png)\n",
    "\n",
    "Horror :\n",
    "\n",
    "![Horror](./img/freq_word/horror%20.png)\n",
    "\n",
    "Si vous voullez voir les mots les plus fréquents par genre, vous pouvez aller dans le dossier img/freq_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def word_frequeny(df):\n",
    "    texts_genre = {}\n",
    "\n",
    "    for i in df.index:\n",
    "        genre = df[\"genre\"][i]\n",
    "        if genre not in texts_genre:\n",
    "            texts_genre[genre] = df[\"description\"][i]\n",
    "        else:\n",
    "            texts_genre[genre] += \" \"\n",
    "            texts_genre[genre] += df[\"description\"][i]\n",
    "\n",
    "    for text in texts_genre.values():\n",
    "\n",
    "        data_analysis = nltk.FreqDist(text.split(\" \"))\n",
    "\n",
    "        filter_words = dict([(m, n)\n",
    "                            for m, n in data_analysis.items() if len(m) > 3])\n",
    "\n",
    "        data_analysis = nltk.FreqDist(filter_words)\n",
    "\n",
    "        data_analysis.plot(10, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Première idée de classifieur (champ lexical)\n",
    "\n",
    "Après avoir calculer la fréquence des mots par genre, nous nous sommes donc orienté vers une classification par rapport au champ lexical.\n",
    "\n",
    "Ce classifieur nous a induit a compté le nombre d'apparitions de mots par genre et ensuite déterminé les mots qui apparaissent le plus par genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import trange\n",
    "\n",
    "def clean_lexical(df):\n",
    "    lexical = df[\"lexical_field\"]\n",
    "    # new_df = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        splited = lexical[i].split(\" \")\n",
    "        l = clean_text(\" \".join(set(splited)))\n",
    "        df[\"lexical_field\"][i] = l\n",
    "\n",
    "    df.to_csv(\"archive/lexical_field_clean.csv\")\n",
    "\n",
    "\n",
    "\n",
    "def similar(summary, lexical_field):\n",
    "    summary = summary.split(\" \")\n",
    "    lexical_field = lexical_field.split(\" \")\n",
    "\n",
    "    ret = 0\n",
    "    for s in summary:\n",
    "        if s in lexical_field:\n",
    "            ret += 1\n",
    "    return ret\n",
    "\n",
    "# partially working version\n",
    "def get_similarities_counter(df, text):\n",
    "    lexical = df[\"lexical_field\"]\n",
    "\n",
    "    genre=\"\"\n",
    "    note=0\n",
    "    clean_test = clean_text(text)\n",
    "    for i in range(len(df)):\n",
    "        tmp=similar(clean_test, lexical[i])\n",
    "        if note < tmp:\n",
    "            note = tmp\n",
    "            genre=df[\"genre\"][i]\n",
    "\n",
    "    #print(genre)\n",
    "    return genre\n",
    "\n",
    "\n",
    "# not working version\n",
    "def get_similarities_fuzzy(df, text):\n",
    "    lexical = df[\"lexical_field\"]\n",
    "\n",
    "    genre=\"\"\n",
    "    note=0\n",
    "    clean_test = clean_text(text)\n",
    "    for i in range(len(df)):\n",
    "        tmp=fuzz.ratio(clean_test, lexical[i])\n",
    "        if note < tmp:\n",
    "            note = tmp\n",
    "            genre=df[\"genre\"][i]\n",
    "\n",
    "    #print(genre)\n",
    "    return genre\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"archive/lexical_field_clean.csv\")\n",
    "clean_lexical(df)\n",
    "data = pd.read_csv(\"archive/dataset_csv/test_data_solution.csv\")\n",
    "\n",
    "sentences = data[\"description\"].values\n",
    "\n",
    "genrePredit=\"\"\n",
    "error=0\n",
    "\n",
    "for i in trange(len(sentences)):\n",
    "    genrePredit=get_similarities_counter(df, sentences[i])\n",
    "    if genrePredit!=data[\"genre\"][i]:\n",
    "        error+=1\n",
    "        # print(genrePredit+\" \"+data[\"genre\"][i])\n",
    "\n",
    "error=(error/len(sentences))*100\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Le main ouvre la base de donnée, la li puis compte la fréquences des mots par genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "    # word_frequeny()\n",
    "    # with open(\"text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #     text = \" \".join(f.readlines())\n",
    "\n",
    "    # df = pd.read_csv(\"archive/dataset_csv/test_data_solution_clean.csv\")\n",
    "\n",
    "    # word_frequeny(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainClassifier\n",
    "\n",
    "Cette class nous permet d'entrainer un classifieur sur un corpus de données.\n",
    "\n",
    "Nous pouvons choisir le modèle de classifieur à utiliser, le corpus de données à utiliser, le nombre de classes à distinguer, le nombre de fois que l'on souhaite entrainer le modèle, le nombre de fois que l'on souhaite tester le modèle et le nombre de fois que l'on souhaite évaluer le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainClassifier(object):\n",
    "    def __init__(self, df_path=\"archive/dataset_csv/train_data_clean.csv\", test_size=0.2, train_size=0.7, sampling=None):\n",
    "        self.vectorizer = None\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.classifier = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.test_size = test_size\n",
    "        self.train_size = train_size\n",
    "        self.sentences = self.df[\"description\"].values\n",
    "        self.Y = self.df[\"genre\"].values\n",
    "        self.sampling = sampling if sampling is not None else None\n",
    "\n",
    "     def train(self, classifier, model_path=\"neural_network_model.sav\", **kwargs):\n",
    "        print(f\"Start training with {classifier}...\")\n",
    "\n",
    "        if self.sampling is not None:\n",
    "            self.sentences = shuffle(self.sentences, n_samples=self.sampling)\n",
    "            self.Y = shuffle(self.Y, n_samples=self.sampling)\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(self.sentences)\n",
    "        self.vectorizer.transform(self.sentences)\n",
    "\n",
    "        sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "            self.sentences, self.Y, test_size=self.test_size, train_size=self.train_size)  # , random_state=50\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(sentences_train)\n",
    "\n",
    "        X_train = self.vectorizer.transform(sentences_train)\n",
    "        X_test = self.vectorizer.transform(sentences_test)\n",
    "\n",
    "        self.classifier = classifier(**kwargs)\n",
    "\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "\n",
    "        save_model(dict(classifier=self.classifier, vectorizer=self.vectorizer),\n",
    "                   model_path)\n",
    "\n",
    "        score = self.classifier.score(X_test, y_test)\n",
    "        print(f\"Trained with {score:.3f}% accuracy\")\n",
    "        metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "            self.classifier, X_test, y_test, labels=self.df[\"genre\"].unique(), include_values=False, xticks_rotation='vertical')\n",
    "        plt.savefig(f'confusion_matrix_{self.classifier}.png', dpi=400)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarder et charger un modèle pré-entrainé\n",
    "\n",
    "Ces fonctions nous permettent de charger un modèle ou de sauvegardé un modèle.\n",
    "Grace à ces fonctions nous ne sommes pas obligé d’entrainer notre réseau de neurones à chaque utilisation, il nous suffit de charger celui qui a était généré préalablement. \n",
    "\n",
    "Charger un modèle permet également de garantir le meilleur résultat que nous ayons trouver pour se classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    pickle.dump(model, open(file_path, \"wb+\"))\n",
    "    print(f\"{file_path} saved\")\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "    model = pickle.load(open(file_path, \"rb\"))\n",
    "    print(f\"{model} loaded\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests du classifieur par SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_train_svc(df_trained_data_path, df_to_predict):\n",
    "    test_sizes = [x/10 for x in range(1, 10)]\n",
    "    train_sizes = [x/10 for x in range(1, 10)]\n",
    "\n",
    "    x_labels = [0]\n",
    "    x_ticks = [0]\n",
    "    i = 0\n",
    "    for a in test_sizes:\n",
    "        for b in train_sizes:\n",
    "            x_labels.append(\"(\"+str(a)+\",\"+str(b)+\")\")\n",
    "            i += 1\n",
    "            x_ticks.append(i)\n",
    "    i = 1\n",
    "\n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "    plt.xlabel(\"(test_size, train_size)\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.title(\"SVM variation\")\n",
    "    plt.grid(True)\n",
    "    x = []\n",
    "    y_accuracy = []\n",
    "    y_error = []\n",
    "\n",
    "    nbOfGraph = 0\n",
    "    for test_size in test_sizes:\n",
    "        for train_size in train_sizes:\n",
    "            if test_size+train_size < 1:\n",
    "                print(str(test_size)+\" \"+str(train_size))\n",
    "                print(x_ticks[i])\n",
    "                print(x_labels[i])\n",
    "\n",
    "                trainer = TrainClassifier(\n",
    "                    df_trained_data_path, test_size, train_size)\n",
    "                score = trainer.train(classifier=LinearSVC, model_path=\"svm_model.sav\")\n",
    "\n",
    "                predict(\"predicted_svm.csv\",\n",
    "                        df_to_predict, classifier_method=\"svm\")\n",
    "                error = predictError(pd.read_csv(\n",
    "                    \"predicted_svm.csv\"))\n",
    "                y_error.append(error)\n",
    "\n",
    "                y_accuracy.append(score*100)\n",
    "                x.append(x_ticks[i])\n",
    "                print(\"Accuracy:\", score)\n",
    "\n",
    "            if i != 0 and i % 9 == 0:\n",
    "                plt.scatter(x, y_accuracy)\n",
    "                plt.scatter(x, y_error)\n",
    "                plt.legend([\"Accuracy\", \"Error\"])\n",
    "\n",
    "                plt.savefig(\n",
    "                    f\"svm-variation-{nbOfGraph}.png\")\n",
    "                y_accuracy = []\n",
    "                y_error = []\n",
    "                x = []\n",
    "                plt.clf()\n",
    "                plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "                plt.xlabel(\"(test_size, train_size)\")\n",
    "                plt.ylabel(\"Percentage (%)\")\n",
    "                plt.title(\"SVM variation\")\n",
    "                plt.grid(True)\n",
    "\n",
    "                nbOfGraph += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests du classifieur par LogisticRegression\n",
    "\n",
    "Ici nous testons différents paramètres de notre classifieur, notamment le nombre de classes à distinguer, le nombre de fois que l'on souhaite entrainer le modèle, le nombre de fois que l'on souhaite tester le modèle et le nombre de fois que l'on souhaite évaluer le modèle.\n",
    "\n",
    "Enfin, nous affichons les résultats de notre classifieur à l'aide de graphs pour détecter les paramètres qui ont le meilleur score.\n",
    "\n",
    "Voici le meilleur résultat obtenu parmis ceux réalisé par nos tests: (test_size=0.2, train_size=0.7) avec une accuracy de 58% et un taux d'erreur d'environ 42%.\n",
    "\n",
    "![Logistic Regression graph tests](./graph_tests/logistic_regression/variation-error_accuracy-1.png)\n",
    "\n",
    "Si vous voulez consultez les autres tests, vous pouvez aller dans le dossier graph_tests/logistic_regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_train_naif_logistic_regression(df_trained_data_path, df_to_predict):\n",
    "    test_sizes = [x/10 for x in range(1, 10)]\n",
    "    train_sizes = [x/10 for x in range(1, 10)]\n",
    "\n",
    "    x_labels = [0]\n",
    "    x_ticks = [0]\n",
    "    i = 0\n",
    "    for a in test_sizes:\n",
    "        for b in train_sizes:\n",
    "            x_labels.append(\"(\"+str(a)+\",\"+str(b)+\")\")\n",
    "            i += 1\n",
    "            x_ticks.append(i)\n",
    "    i = 1\n",
    "\n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "    plt.xlabel(\"(test_size, train_size)\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.title(\"Naïf Logistic Regression variation\")\n",
    "    plt.grid(True)\n",
    "    x = []\n",
    "    y_accuracy = []\n",
    "    y_error = []\n",
    "\n",
    "    nbOfGraph = 0\n",
    "    for test_size in test_sizes:\n",
    "        for train_size in train_sizes:\n",
    "            if test_size+train_size < 1:\n",
    "                print(str(test_size)+\" \"+str(train_size))\n",
    "                print(x_ticks[i])\n",
    "                print(x_labels[i])\n",
    "\n",
    "                trainer = TrainClassifier(\n",
    "                    df_trained_data_path, test_size, train_size)\n",
    "                score = trainer.train(classifier=LogisticRegression, model_path=\"naif_logistic_regression_model.sav\")\n",
    "\n",
    "                predict(\"predicted_logistic_regression.csv\",\n",
    "                        df_to_predict, classifier_method=\"logistic_regression\")\n",
    "                error = predictError(pd.read_csv(\n",
    "                    \"predicted_logistic_regression.csv\"))\n",
    "                y_error.append(error)\n",
    "\n",
    "                y_accuracy.append(score*100)\n",
    "                x.append(x_ticks[i])\n",
    "                print(\"Accuracy:\", score)\n",
    "\n",
    "            if i != 0 and i % 9 == 0:\n",
    "                plt.scatter(x, y_accuracy)\n",
    "                plt.scatter(x, y_error)\n",
    "                plt.legend([\"Accuracy\", \"Error\"])\n",
    "\n",
    "                plt.savefig(\n",
    "                    f\"variation-error_accuracy-{nbOfGraph}.png\")\n",
    "                y_accuracy = []\n",
    "                y_error = []\n",
    "                x = []\n",
    "                plt.clf()\n",
    "                plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "                plt.xlabel(\"(test_size, train_size)\")\n",
    "                plt.ylabel(\"Percentage (%)\")\n",
    "                plt.title(\"Naïf Logistic Regression variation\")\n",
    "                plt.grid(True)\n",
    "\n",
    "                nbOfGraph += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests du classifieur par MLPClassifier\n",
    "\n",
    "Ici nous testons différents paramètres de notre classifieur, notamment le nombre de layers et le nombre de neurones par layer.\n",
    "\n",
    "Enfin, nous affichons les résultats de notre classifieur à l'aide de graphs pour détecter les paramètres qui ont le meilleur score.\n",
    "\n",
    "Voici le meilleur résultat obtenu parmis ceux réalisé par nos tests: (layers=1, neurones=100) avec une accuracy de 58% et un taux d'erreur d'environ 42%.\n",
    "\n",
    "![](./graph_tests/mlp_classifier/neural_network_variation-error_accuracy-0.png)\n",
    "\n",
    "Vous pouvez aussi consultez les autres tests, vous pouvez aller dans le dossier graph_tests/mlp_classifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_train_neural_network(df_trained_data_path, df_to_predict):\n",
    "    x_labels = [0]\n",
    "    x_ticks = [0]\n",
    "    i = 0\n",
    "    for layer in range(1, 5):\n",
    "        for neuron in range(1, 101, 9):\n",
    "            x_labels.append(str(layer)+\" x \"+str(neuron))\n",
    "            i += 1\n",
    "            x_ticks.append(i)\n",
    "    i = 1\n",
    "\n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "    plt.xlabel(\"layer X neurons\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.title(\"Neural Network MLPClassifier variation\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    x = []\n",
    "    y_accuracy = []\n",
    "    y_error = []\n",
    "\n",
    "    nbOfGraph = 0\n",
    "    for layer in range(1, 5):\n",
    "        for neuron in range(1, 101, 9):\n",
    "            print(str(layer)+\" x \"+str(neuron))\n",
    "            print(x_ticks[i])\n",
    "            print(x_labels[i])\n",
    "            trainer = TrainClassifier(\n",
    "                df_trained_data_path, 0.2, 0.7)\n",
    "            score = trainer.train(classifier=MLPClassifier,\n",
    "                                  hidden_layer_sizes=tuple((neuron for i in range(layer))))\n",
    "\n",
    "            predict(\"predicted_neural_network.csv\",\n",
    "                    df_to_predict, classifier_method=\"neural_network\")\n",
    "            error = predictError(pd.read_csv(\n",
    "                \"predicted_neural_network.csv\"))\n",
    "\n",
    "            y_error.append(error)\n",
    "            y_accuracy.append(score*100)\n",
    "            x.append(x_ticks[i])\n",
    "\n",
    "            print(\"Accuracy:\", score)\n",
    "            i += 1\n",
    "\n",
    "        plt.scatter(x, y_accuracy)\n",
    "        plt.scatter(x, y_error)\n",
    "        plt.legend([\"Accuracy\", \"Error\"])\n",
    "        plt.savefig(\n",
    "            f\"neural_network_variation-error_accuracy-{nbOfGraph}.png\")\n",
    "        y_accuracy = []\n",
    "        y_error = []\n",
    "        x = []\n",
    "        plt.clf()\n",
    "\n",
    "        plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "        plt.xlabel(\"layer X neurons\")\n",
    "        plt.ylabel(\"Percentage (%)\")\n",
    "        plt.title(\"Neural Network MLPClassifier variation\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        nbOfGraph += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction \n",
    "\n",
    "Pour prédire le genre d’un film nous commençons par récupérer un modèle entrainé, nous vectorisons la description puis nous appelons la fonction predict du classifieur choisie.\n",
    "Nous pouvons ainsi calculer le taux d’erreur grâce à predictError.\n",
    "\n",
    "La fonction convert_string_to_dataset_prediction quant à elle sert uniquement à générer un CSV qui pourra être traité par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictError(df):\n",
    "    genre = df[\"genre\"]\n",
    "    genre_predicted = df[\"genre_predit\"]\n",
    "    length_genre = len(genre)\n",
    "    error = 0\n",
    "    for i in range(length_genre):\n",
    "        if genre[i] != genre_predicted[i]:\n",
    "            error += 1\n",
    "\n",
    "    print(f\"Error : {(error/length_genre)*100:.3f}%\")\n",
    "    return (error/length_genre)*100\n",
    "\n",
    "\n",
    "def predict(output, df_to_predict, classifier_method=\"neural_network\", sampling=None, model_path=\"neural_network_model.sav\", **kwargs):\n",
    "\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "    except:\n",
    "        trainer = TrainClassifier(sampling=sampling)\n",
    "        if classifier_method == \"neural_network\":\n",
    "            trainer.train(MLPClassifier, **kwargs)\n",
    "        elif classifier_method == \"logistic_regression\":\n",
    "            trainer.train(LogisticRegression, **kwargs)\n",
    "        elif classifier_method == \"svm\":\n",
    "            trainer.train(LinearSVC, **kwargs)\n",
    "\n",
    "        model = load_model(model_path)\n",
    "\n",
    "    classifier = model[\"classifier\"]\n",
    "    vectorizer = model[\"vectorizer\"]\n",
    "    print(f\"Predicting with {classifier}...\")\n",
    "\n",
    "    X_test_to_predict = vectorizer.transform(df_to_predict[\"description\"])\n",
    "\n",
    "    classified = classifier.predict(X_test_to_predict)\n",
    "    try:\n",
    "        # calcul du pourcentage d'erreurs\n",
    "        df2 = pd.DataFrame(data={\n",
    "            \"title\": df_to_predict[\"title\"].values, \"description\": df_to_predict[\"description\"], \"genre\": df_to_predict[\"genre\"], \"genre_predit\": classified})\n",
    "    except:\n",
    "        df2 = pd.DataFrame(data={\n",
    "            \"title\": df_to_predict[\"title\"].values, \"description\": df_to_predict[\"description\"], \"genre_predit\": classified})\n",
    "\n",
    "    # predictError(df2)\n",
    "    df2.to_csv(output)\n",
    "    print(f\"Prediction done and saved in {model_path}\")\n",
    "\n",
    "\n",
    "def convert_string_to_dataset_prediction(title, description):\n",
    "    return pd.DataFrame({\"title\": title, \"description\": clean_text(description)}, index=[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Le main est la fonction principal du code. Celle qui va lancer les autres fonctions afin de prédire le genre d’un film grâce à son résumé.\n",
    "C’est également ici qu’on parse les arguments afin de pouvoir personnaliser son éxpérience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7590/2008562771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# df_trained_data = pd.read_csv(\"archive/dataset_csv/train_data_clean.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf_to_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_string_to_dataset_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Junoon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"A wannabe vlogger travels from Saudi Arabia with his wife and best friend all the way to Southern California, wishing for some great paranormal footage. When their wish comes true, will they know when to turn the cameras off and flee?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# df_to_predict = pd.read_csv(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#     \"archive/dataset_csv/test_data_solution_clean.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7590/2654520420.py\u001b[0m in \u001b[0;36mconvert_string_to_dataset_prediction\u001b[0;34m(title, description)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_string_to_dataset_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"description\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # df_trained_data = pd.read_csv(\"archive/dataset_csv/train_data_clean.csv\")\n",
    "\n",
    "    df_to_predict=convert_string_to_dataset_prediction(\"Junoon\", \"A wannabe vlogger travels from Saudi Arabia with his wife and best friend all the way to Southern California, wishing for some great paranormal footage. When their wish comes true, will they know when to turn the cameras off and flee?\")\n",
    "    # df_to_predict = pd.read_csv(\n",
    "    #     \"archive/dataset_csv/test_data_solution_clean.csv\")\n",
    "    # train(df_trained_data, 0.1, 0.6)\n",
    "    predict(\"junoon-prediction.csv\", df_to_predict=df_to_predict, classifier_method=\"svm\", model_path=\"svm_model.sav\")\n",
    "\n",
    "    #score = train(df_trained_data, 2/3, 1/3)\n",
    "    # graph_train_naif_logistic_regression(\n",
    "    #     \"archive/dataset_csv/train_data_clean.csv\", df_to_predict)\n",
    "\n",
    "    # graph_train_neural_network(\n",
    "    #     \"archive/dataset_csv/train_data_clean.csv\", df_to_predict)\n",
    "\n",
    "    #print(\"Accuracy:\", score)\n",
    "    # parser = argparse.ArgumentParser(description='Predict genre of a movie')\n",
    "    # parser.add_argument('-t', '--title', type=str,\n",
    "    #                     help=\"Title of the movie\", dest=\"title\", default=\"\")\n",
    "    # parser.add_argument('-d', '--description', type=str,\n",
    "    #                     help=\"Description of the movie\", dest=\"description\", default=None)\n",
    "\n",
    "    # parser.add_argument('-cl', '--classifier', type=str,\n",
    "    #                     help=\"Classifier to use\", dest=\"classifier\", default=\"neural_network\")\n",
    "    # parser.add_argument('-mp', '--model_path', type=str, help=\"Path to the model\", dest=\"model_path\", default=\"neural_network_model.sav\")\n",
    "    # parser.add_argument('-tr', '--train', type=bool, help=\"Train the model\", dest=\"train\", default=False)\n",
    "    # parser.add_argument('-csv', '--csv_output', type=str, help=\"CSV output file\", dest=\"csv_output\", default=\"predicted.csv\")\n",
    "    # parser.add_argument('-n', '--neurons', type=int, help=\"Number of neurons in the hidden layer\", dest=\"neurons\", default=100)\n",
    "    # parser.add_argument('-l', '--layers', type=int, help=\"Number of hidden layers\", dest=\"layers\", default=1)\n",
    "    # args = parser.parse_args()\n",
    "    # print(args)\n",
    "\n",
    "\n",
    "    # if args.train:\n",
    "    #     trainer = TrainClassifier()\n",
    "    #     if args.classifier == \"neural_network\":\n",
    "    #         trainer.train_neural_network(hidden_layer_sizes=tuple((args.neurons for i in range(args.layers))))\n",
    "    #     elif args.classifier == \"logistic_regression\":\n",
    "    #         trainer.train_naif_logistic_regression()\n",
    "    #     else:\n",
    "    #         print(\"Unknown classifier\")\n",
    "    #         sys.exit(1)\n",
    "\n",
    "    # if args.description is not None:\n",
    "    #     predict(args.csv_output, convert_string_to_dataset_prediction(args.title, args.description), classifier_method=args.classifier, model_path=args.model_path)\n",
    "\n",
    "\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
