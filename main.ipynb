{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage du text\n",
    "\n",
    "Etapes de néttoyage:\n",
    "1. Mettre en minuscle le texte\n",
    "2. Enlever les contractions\n",
    "3. Enlever les espaces\n",
    "4. Tokeniser le texte\n",
    "5. Lemmatiser les mots\n",
    "6. Enlever les éléments ininteréssant (chiffres, ponctuation, mots non anglais et lettres isolées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dune\n",
      " The story of Paul Atreides, a young man as gifted as he was brilliant, destined to meet an extraordinary destiny that totally surpasses him. Because if he wants to preserve the future of his family and his people, he will have to go to the most dangerous planet in the universe - the only one able to provide the most precious resource in the world, capable of multiplying tenfold. power of mankind. As evil forces vie for control of this planet, only those who manage to overcome their fear will be able to survive ...\n",
      "-----------------------------------------------------\n",
      "dune story paul atreides young man gifted wa brilliant destined meet extraordinary destiny totally surpasses want preserve future family people dangerous planet universe able provide precious resource world capable multiplying tenfold power mankind evil force vie control planet manage overcome fear able survive\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "stopwords_list += ['one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
    "                   'eight', 'nine', 'go', 'goes', 'get', 'also', 'however', 'tells']\n",
    "stopwords_list += [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "stopwords_list += [x for x in string.ascii_lowercase]\n",
    "\n",
    "\n",
    "def clean_text(text: str):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('\\d+', ' ', text)\n",
    "\n",
    "    text = text.strip(' ')\n",
    "    text = nltk.word_tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stem_sentence = []\n",
    "    for word in text:\n",
    "        stem = lemmatizer.lemmatize(word)\n",
    "        stem_sentence.append(stem)\n",
    "\n",
    "    text = [w for w in stem_sentence if w not in stopwords_list]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # to_csv(\"archive/dataset/test_data.txt\", \"test_data_clean.csv\")\n",
    "    # to_csv_train_solution(\"archive/dataset/test_data_solution.txt\", \"test_data_solution_clean.csv\")\n",
    "    with open(\"text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = \" \".join(f.readlines())\n",
    "    print(text)\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    cleaned_text = clean_text(text)\n",
    "    print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage de la base de donnée\n",
    "\n",
    "L'une des première étapes étant de nettoyer notre base de données.\n",
    "\n",
    "Pour se faire nous avons transformer la base de donnée de base en txt vers csv tout en nettoyant le text avec la fonction clean_text pour que ce soit plus facile à manipuler.\n",
    "\n",
    "**Base de donnée txt:**\n",
    "\n",
    "<img src=\"img/clean_db/db_txt.PNG\">\n",
    "\n",
    "**Base de donnée csv nettoyée:**\n",
    "\n",
    "<img src=\"img/clean_db/db_csv_clean.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(file, output):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    df = pd.DataFrame(columns=[\"id\", \"title\", \"description\"])\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].split(\":::\")\n",
    "\n",
    "        df_tmp = pd.DataFrame([(int(line[0]), line[1], clean_text(line[2]))], columns=[\n",
    "                              \"id\", \"title\", \"description\"])\n",
    "        df = df.append(df_tmp)\n",
    "    df.to_csv(output)\n",
    "    print(output, \"Saved\")\n",
    "\n",
    "\n",
    "def to_csv_train_solution(file, output):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    df = pd.DataFrame(columns=[\"id\", \"title\", \"genre\", \"description\"])\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i].split(\":::\")\n",
    "\n",
    "        df_tmp = pd.DataFrame([(int(line[0]), line[1], line[2], clean_text(\n",
    "            line[3]))], columns=[\"id\", \"title\", \"genre\", \"description\"])\n",
    "        df = df.append(df_tmp)\n",
    "    df.to_csv(output)\n",
    "    print(output, \"Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fréquence des mots\n",
    "\n",
    "En regardant la fréquence des mots par genre nous avons pu remarquer qu’il varié selon ce dernier.\n",
    "\n",
    "Ça nous a donc donné notre première idée de classifieur, la classification par champs lexical.\n",
    "\n",
    "Exemples de fréquence de mot par genre:\n",
    "\n",
    "Drama :\n",
    "\n",
    "<img src=\"img/freq_word/drama%20.png\">\n",
    "\n",
    "Comedy :\n",
    "\n",
    "<img src=\"img/freq_word/comedy%20.png\">\n",
    "\n",
    "Horror :\n",
    "\n",
    "<img src=\"img/freq_word/horror%20.png\">\n",
    "\n",
    "\n",
    "Si vous voullez voir les mots les plus fréquents par genre, vous pouvez aller dans le dossier img/freq_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequeny(df):\n",
    "    texts_genre = {}\n",
    "\n",
    "    for i in df.index:\n",
    "        genre = df[\"genre\"][i]\n",
    "        if genre not in texts_genre:\n",
    "            texts_genre[genre] = df[\"description\"][i]\n",
    "        else:\n",
    "            texts_genre[genre] += \" \"\n",
    "            texts_genre[genre] += df[\"description\"][i]\n",
    "\n",
    "    for text in texts_genre.values():\n",
    "\n",
    "        data_analysis = nltk.FreqDist(text.split(\" \"))\n",
    "\n",
    "        filter_words = dict([(m, n)\n",
    "                            for m, n in data_analysis.items() if len(m) > 3])\n",
    "\n",
    "        data_analysis = nltk.FreqDist(filter_words)\n",
    "        \n",
    "        data_analysis.plot(10, cumulative=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Première idée de classifieur (champ lexical)\n",
    "\n",
    "Après avoir calculer la fréquence des mots par genre, nous nous sommes donc orienté vers une classification par rapport au champ lexical.\n",
    "\n",
    "Ce classifieur nous a induit a compté le nombre d'apparitions de mots par genre et ensuite déterminé les mots qui apparaissent le plus par genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def clean_lexical(df):\n",
    "    lexical = df[\"lexical_field\"]\n",
    "    # new_df = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        splited = lexical[i].split(\" \")\n",
    "        l = clean_text(\" \".join(set(splited)))\n",
    "        df[\"lexical_field\"][i] = l\n",
    "\n",
    "    df.to_csv(\"archive/lexical_field_clean.csv\")\n",
    "\n",
    "\n",
    "def similar(summary, lexical_field):\n",
    "    summary = summary.split(\" \")\n",
    "    lexical_field = lexical_field.split(\" \")\n",
    "\n",
    "    ret = 0\n",
    "    for s in summary:\n",
    "        if s in lexical_field:\n",
    "            ret += 1\n",
    "    return ret\n",
    "\n",
    "# partially working version\n",
    "\n",
    "\n",
    "def get_similarities_counter(df, text):\n",
    "    lexical = df[\"lexical_field\"]\n",
    "\n",
    "    genre = \"\"\n",
    "    note = 0\n",
    "    clean_test = clean_text(text)\n",
    "    for i in range(len(df)):\n",
    "        tmp = similar(clean_test, lexical[i])\n",
    "        if note < tmp:\n",
    "            note = tmp\n",
    "            genre = df[\"genre\"][i]\n",
    "\n",
    "    # print(genre)\n",
    "    return genre\n",
    "\n",
    "# df = pd.read_csv(\"./archive/lexical_field_clean.csv\")\n",
    "# clean_lexical(df)\n",
    "# data = pd.read_csv(\"./archive/dataset_csv/test_data_solution.csv\")\n",
    "\n",
    "# sentences = data[\"description\"].values\n",
    "\n",
    "# genrePredit=\"\"\n",
    "# error=0\n",
    "\n",
    "# for i in trange(len(sentences)):\n",
    "#     genrePredit=get_similarities_counter(df, sentences[i])\n",
    "#     if genrePredit!=data[\"genre\"][i]:\n",
    "#         error+=1\n",
    "#         # print(genrePredit+\" \"+data[\"genre\"][i])\n",
    "\n",
    "# error=(error/len(sentences))*100\n",
    "# print(error)\n",
    "# if __name__ == \"__main__\":\n",
    "#     with open(\"text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#         text = \" \".join(f.readlines())\n",
    "\n",
    "#     df = pd.read_csv(\"archive/dataset_csv/test_data_solution_clean.csv\")\n",
    "\n",
    "#     word_frequeny(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainClassifier\n",
    "\n",
    "Cette class nous permet d'entrainer un classifieur sur un corpus de données.\n",
    "\n",
    "Nous pouvons choisir le modèle de classifieur à utiliser, le corpus de données à utiliser, le nombre de classes à distinguer, le nombre de fois que l'on souhaite entrainer le modèle, le nombre de fois que l'on souhaite tester le modèle et le nombre de fois que l'on souhaite évaluer le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainClassifier(object):\n",
    "    def __init__(self, df_path=\"archive/dataset_csv/train_data_clean.csv\", test_size=0.1, train_size=0.5, sampling=None):\n",
    "        self.vectorizer = None\n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.classifier = None\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.test_size = test_size\n",
    "        self.train_size = train_size\n",
    "        self.sentences = self.df[\"description\"].values\n",
    "        self.Y = self.df[\"genre\"].values\n",
    "        self.sampling = sampling if sampling is not None else None\n",
    "\n",
    "    def train(self, classifier, model_path=\"neural_network_model.sav\", **kwargs):\n",
    "        print(f\"Start training with {classifier}...\")\n",
    "\n",
    "        if self.sampling is not None:\n",
    "            self.sentences = shuffle(self.sentences, n_samples=self.sampling)\n",
    "            self.Y = shuffle(self.Y, n_samples=self.sampling)\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(self.sentences)\n",
    "        self.vectorizer.transform(self.sentences)\n",
    "\n",
    "        sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "            self.sentences, self.Y, test_size=self.test_size, train_size=self.train_size)  # , random_state=50\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(sentences_train)\n",
    "\n",
    "        X_train = self.vectorizer.transform(sentences_train)\n",
    "        X_test = self.vectorizer.transform(sentences_test)\n",
    "\n",
    "        self.classifier = classifier(**kwargs)\n",
    "\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "\n",
    "        save_model(dict(classifier=self.classifier, vectorizer=self.vectorizer),\n",
    "                   model_path)\n",
    "\n",
    "        score = self.classifier.score(X_test, y_test)\n",
    "        print(f\"Trained with {score:.3f}% accuracy\")\n",
    "        metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "            self.classifier, X_test, y_test, labels=self.df[\"genre\"].unique(), include_values=False, xticks_rotation=90)\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarder et charger un modèle pré-entrainé\n",
    "\n",
    "Ces fonctions nous permettent de charger un modèle ou de sauvegardé un modèle.\n",
    "Grace à ces fonctions nous ne sommes pas obligé d’entrainer notre réseau de neurones à chaque utilisation, il nous suffit de charger celui qui a était généré préalablement. \n",
    "\n",
    "Charger un modèle permet également de garantir le meilleur résultat que nous ayons trouver pour se classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    pickle.dump(model, open(file_path, \"wb+\"))\n",
    "    print(f\"{file_path} saved\")\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "    model = pickle.load(open(file_path, \"rb\"))\n",
    "    print(f\"{model} loaded\")\n",
    "    print(model[\"vectorizer\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests du classifieur par SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def graph_train_svc(df_trained_data_path, df_to_predict):\n",
    "    test_sizes = [x/10 for x in range(1, 10)]\n",
    "    train_sizes = [x/10 for x in range(1, 10)]\n",
    "\n",
    "    x_labels = [0]\n",
    "    x_ticks = [0]\n",
    "    i = 0\n",
    "    for a in test_sizes:\n",
    "        for b in train_sizes:\n",
    "            x_labels.append(\"(\"+str(a)+\",\"+str(b)+\")\")\n",
    "            i += 1\n",
    "            x_ticks.append(i)\n",
    "    i = 1\n",
    "\n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "    plt.xlabel(\"(test_size, train_size)\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.title(\"SVM variation\")\n",
    "    plt.grid(True)\n",
    "    x = []\n",
    "    y_accuracy = []\n",
    "    y_error = []\n",
    "\n",
    "    nbOfGraph = 0\n",
    "    for test_size in test_sizes:\n",
    "        for train_size in train_sizes:\n",
    "            if test_size+train_size < 1:\n",
    "                print(str(test_size)+\" \"+str(train_size))\n",
    "                print(x_ticks[i])\n",
    "                print(x_labels[i])\n",
    "\n",
    "                trainer = TrainClassifier(\n",
    "                    df_trained_data_path, test_size, train_size)\n",
    "                score = trainer.train(\n",
    "                    classifier=LinearSVC, model_path=\"svm_model.sav\")\n",
    "\n",
    "                predict(\"predicted_svm.csv\",\n",
    "                        df_to_predict, classifier_method=\"svm\")\n",
    "                error = predictError(pd.read_csv(\n",
    "                    \"predicted_svm.csv\"))\n",
    "                y_error.append(error)\n",
    "\n",
    "                y_accuracy.append(score*100)\n",
    "                x.append(x_ticks[i])\n",
    "                print(\"Accuracy:\", score)\n",
    "\n",
    "            if i != 0 and i % 9 == 0:\n",
    "                plt.scatter(x, y_accuracy)\n",
    "                plt.scatter(x, y_error)\n",
    "                plt.legend([\"Accuracy\", \"Error\"])\n",
    "\n",
    "                plt.savefig(\n",
    "                    f\"svm-variation-{nbOfGraph}.png\")\n",
    "                y_accuracy = []\n",
    "                y_error = []\n",
    "                x = []\n",
    "                plt.clf()\n",
    "                plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "                plt.xlabel(\"(test_size, train_size)\")\n",
    "                plt.ylabel(\"Percentage (%)\")\n",
    "                plt.title(\"SVM variation\")\n",
    "                plt.grid(True)\n",
    "\n",
    "                nbOfGraph += 1\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests du classifieur par LogisticRegression\n",
    "\n",
    "Ici nous testons différents paramètres de notre classifieur, notamment le nombre de classes à distinguer, le nombre de fois que l'on souhaite entrainer le modèle, le nombre de fois que l'on souhaite tester le modèle et le nombre de fois que l'on souhaite évaluer le modèle.\n",
    "\n",
    "Enfin, nous affichons les résultats de notre classifieur à l'aide de graphs pour détecter les paramètres qui ont le meilleur score.\n",
    "\n",
    "Voici le meilleur résultat obtenu parmis ceux réalisé par nos tests: (test_size=0.2, train_size=0.7) avec une accuracy de 58% et un taux d'erreur d'environ 42%.\n",
    "\n",
    "<img src=\"graph_tests/logistic_regression/variation-error_accuracy-1.png\">\n",
    "\n",
    "Si vous voulez consultez les autres tests, vous pouvez aller dans le dossier graph_tests/logistic_regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_train_naif_logistic_regression(df_trained_data_path, df_to_predict):\n",
    "    test_sizes = [x/10 for x in range(1, 10)]\n",
    "    train_sizes = [x/10 for x in range(1, 10)]\n",
    "\n",
    "    x_labels = [0]\n",
    "    x_ticks = [0]\n",
    "    i = 0\n",
    "    for a in test_sizes:\n",
    "        for b in train_sizes:\n",
    "            x_labels.append(\"(\"+str(a)+\",\"+str(b)+\")\")\n",
    "            i += 1\n",
    "            x_ticks.append(i)\n",
    "    i = 1\n",
    "\n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "    plt.xlabel(\"(test_size, train_size)\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.title(\"Naïf Logistic Regression variation\")\n",
    "    plt.grid(True)\n",
    "    x = []\n",
    "    y_accuracy = []\n",
    "    y_error = []\n",
    "\n",
    "    nbOfGraph = 0\n",
    "    for test_size in test_sizes:\n",
    "        for train_size in train_sizes:\n",
    "            if test_size+train_size < 1:\n",
    "                print(str(test_size)+\" \"+str(train_size))\n",
    "                print(x_ticks[i])\n",
    "                print(x_labels[i])\n",
    "\n",
    "                trainer = TrainClassifier(\n",
    "                    df_trained_data_path, test_size, train_size)\n",
    "                score = trainer.train(\n",
    "                    classifier=LogisticRegression, model_path=\"naif_logistic_regression_model.sav\")\n",
    "\n",
    "                predict(\"predicted_logistic_regression.csv\",\n",
    "                        df_to_predict, classifier_method=\"logistic_regression\")\n",
    "                error = predictError(pd.read_csv(\n",
    "                    \"predicted_logistic_regression.csv\"))\n",
    "                y_error.append(error)\n",
    "\n",
    "                y_accuracy.append(score*100)\n",
    "                x.append(x_ticks[i])\n",
    "                print(\"Accuracy:\", score)\n",
    "\n",
    "            if i != 0 and i % 9 == 0:\n",
    "                plt.scatter(x, y_accuracy)\n",
    "                plt.scatter(x, y_error)\n",
    "                plt.legend([\"Accuracy\", \"Error\"])\n",
    "\n",
    "                plt.savefig(\n",
    "                    f\"variation-error_accuracy-{nbOfGraph}.png\")\n",
    "                y_accuracy = []\n",
    "                y_error = []\n",
    "                x = []\n",
    "                plt.clf()\n",
    "                plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "                plt.xlabel(\"(test_size, train_size)\")\n",
    "                plt.ylabel(\"Percentage (%)\")\n",
    "                plt.title(\"Naïf Logistic Regression variation\")\n",
    "                plt.grid(True)\n",
    "\n",
    "                nbOfGraph += 1\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests du classifieur par MLPClassifier\n",
    "\n",
    "Ici nous testons différents paramètres de notre classifieur, notamment le nombre de layers et le nombre de neurones par layer.\n",
    "\n",
    "Enfin, nous affichons les résultats de notre classifieur à l'aide de graphs pour détecter les paramètres qui ont le meilleur score.\n",
    "\n",
    "Voici le meilleur résultat obtenu parmis ceux réalisé par nos tests: (layers=1, neurones=100) avec une accuracy de 58% et un taux d'erreur d'environ 42%.\n",
    "\n",
    "<img src=\"./graph_tests/mlp_classifier/neural_network_variation-error_accuracy-0.png\">\n",
    "\n",
    "Vous pouvez aussi consultez les autres tests, vous pouvez aller dans le dossier graph_tests/mlp_classifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def graph_train_neural_network(df_trained_data_path, df_to_predict):\n",
    "    x_labels = [0]\n",
    "    x_ticks = [0]\n",
    "    i = 0\n",
    "    for layer in range(1, 5):\n",
    "        for neuron in range(1, 101, 9):\n",
    "            x_labels.append(str(layer)+\" x \"+str(neuron))\n",
    "            i += 1\n",
    "            x_ticks.append(i)\n",
    "    i = 1\n",
    "\n",
    "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "    plt.xlabel(\"layer X neurons\")\n",
    "    plt.ylabel(\"Percentage (%)\")\n",
    "    plt.title(\"Neural Network MLPClassifier variation\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    x = []\n",
    "    y_accuracy = []\n",
    "    y_error = []\n",
    "\n",
    "    nbOfGraph = 0\n",
    "    for layer in range(1, 5):\n",
    "        for neuron in range(1, 101, 9):\n",
    "            print(str(layer)+\" x \"+str(neuron))\n",
    "            print(x_ticks[i])\n",
    "            print(x_labels[i])\n",
    "            trainer = TrainClassifier(\n",
    "                df_trained_data_path, 0.2, 0.7)\n",
    "            score = trainer.train(classifier=MLPClassifier,\n",
    "                                  hidden_layer_sizes=tuple((neuron for i in range(layer))))\n",
    "\n",
    "            predict(\"predicted_neural_network.csv\",\n",
    "                    df_to_predict, classifier_method=\"neural_network\")\n",
    "            error = predictError(pd.read_csv(\n",
    "                \"predicted_neural_network.csv\"))\n",
    "\n",
    "            y_error.append(error)\n",
    "            y_accuracy.append(score*100)\n",
    "            x.append(x_ticks[i])\n",
    "\n",
    "            print(\"Accuracy:\", score)\n",
    "            i += 1\n",
    "\n",
    "        plt.scatter(x, y_accuracy)\n",
    "        plt.scatter(x, y_error)\n",
    "        plt.legend([\"Accuracy\", \"Error\"])\n",
    "        plt.savefig(\n",
    "            f\"neural_network_variation-error_accuracy-{nbOfGraph}.png\")\n",
    "        y_accuracy = []\n",
    "        y_error = []\n",
    "        x = []\n",
    "        plt.clf()\n",
    "\n",
    "        plt.xticks(ticks=x_ticks, labels=x_labels, rotation=10)\n",
    "        plt.xlabel(\"layer X neurons\")\n",
    "        plt.ylabel(\"Percentage (%)\")\n",
    "        plt.title(\"Neural Network MLPClassifier variation\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        nbOfGraph += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction \n",
    "\n",
    "Pour prédire le genre d’un film nous commençons par récupérer un modèle entrainé, nous vectorisons la description puis nous appelons la fonction predict du classifieur choisie.\n",
    "Nous pouvons ainsi calculer le taux d’erreur grâce à predictError.\n",
    "\n",
    "La fonction convert_string_to_dataset_prediction quant à elle sert uniquement à générer un CSV qui pourra être traité par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictError(df):\n",
    "    genre = df[\"genre\"]\n",
    "    genre_predicted = df[\"genre_predit\"]\n",
    "    length_genre = len(genre)\n",
    "    error = 0\n",
    "    for i in range(length_genre):\n",
    "        if genre[i] != genre_predicted[i]:\n",
    "            error += 1\n",
    "\n",
    "    print(f\"Error : {(error/length_genre)*100:.3f}%\")\n",
    "    return (error/length_genre)*100\n",
    "\n",
    "\n",
    "def predict(output, df_to_predict, classifier_method=\"neural_network\", sampling=None, model_path=\"neural_network_model.sav\", **kwargs):\n",
    "\n",
    "    try:\n",
    "        model = load_model(model_path)\n",
    "    except:\n",
    "        trainer = TrainClassifier(sampling=sampling)\n",
    "        if classifier_method == \"neural_network\":\n",
    "            trainer.train(MLPClassifier, **kwargs)\n",
    "        elif classifier_method == \"logistic_regression\":\n",
    "            trainer.train(LogisticRegression, **kwargs)\n",
    "        elif classifier_method == \"svm\":\n",
    "            trainer.train(LinearSVC, **kwargs)\n",
    "\n",
    "        model = load_model(model_path)\n",
    "\n",
    "    classifier = model[\"classifier\"]\n",
    "    vectorizer = model[\"vectorizer\"]\n",
    "    print(f\"Predicting with {classifier}...\")\n",
    "\n",
    "    X_test_to_predict = vectorizer.transform(df_to_predict[\"description\"])\n",
    "\n",
    "    classified = classifier.predict(X_test_to_predict)\n",
    "    try:\n",
    "        # calcul du pourcentage d'erreurs\n",
    "        df2 = pd.DataFrame(data={\n",
    "            \"title\": df_to_predict[\"title\"].values, \"description\": df_to_predict[\"description\"], \"genre\": df_to_predict[\"genre\"], \"genre_predit\": classified})\n",
    "    except:\n",
    "        df2 = pd.DataFrame(data={\n",
    "            \"title\": df_to_predict[\"title\"].values, \"description\": df_to_predict[\"description\"], \"genre_predit\": classified})\n",
    "\n",
    "    # predictError(df2)\n",
    "    df2.to_csv(output)\n",
    "    print(f\"Prediction done and saved in {output}\")\n",
    "    print(f\"Classifier {classifier} prediction for {df_to_predict['title'].values} is {classified}\")\n",
    "\n",
    "\n",
    "def convert_string_to_dataset_prediction(title, description):\n",
    "    # print(clean_text(str(description)))\n",
    "    return pd.DataFrame({\"title\": str(title), \"description\": str(clean_text(description))}, index=[0])\n",
    "\n",
    "\n",
    "def read_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        title, description = map(str.strip, f.readlines())\n",
    "        return title, description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Le main est la fonction principal du code. Celle qui va lancer les autres fonctions afin de prédire le genre d’un film grâce à son résumé.\n",
    "C’est également ici qu’on parse les arguments afin de pouvoir personnaliser son éxpérience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': LinearSVC(), 'vectorizer': TfidfVectorizer()} loaded\n",
      "TfidfVectorizer()\n",
      "Predicting with LinearSVC()...\n",
      "Prediction done and saved in ./predictions/dune.csv\n",
      "Classifier LinearSVC() prediction for ['Dune'] is [' sci-fi ']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_to_predict = convert_string_to_dataset_prediction(\"Dune\", \"The story of Paul Atreides, a young man as gifted as he was brilliant, destined to meet an extraordinary destiny that totally surpasses him. Because if he wants to preserve the future of his family and his people, he will have to go to the most dangerous planet in the universe - the only one able to provide the most precious resource in the world, capable of multiplying tenfold. power of mankind. As evil forces vie for control of this planet, only those who manage to overcome their fear will be able to survive ...\")\n",
    "    output = \"./predictions/dune.csv\"\n",
    "    predict(output, df_to_predict=df_to_predict,\n",
    "            classifier_method=\"svm\", model_path=\"./models/svm.sav\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
